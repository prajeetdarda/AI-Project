{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8921d31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, math, time, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, accuracy_score\n",
    "\n",
    "import torch, torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Paths\n",
    "ROOT = Path.cwd().parents[1]  # 'web/model-training/notebooks' -> ROOT='web'\n",
    "PARQUET = ROOT / \"model-training\" / \"output\" / \"embeddings_panns.parquet\"\n",
    "OUT_DIR = ROOT / \"model-training\" / \"output\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "MODEL_PATH   = OUT_DIR / \"multitask_head.pt\"\n",
    "SCALERS_JSON = OUT_DIR / \"regression_scalers.json\"\n",
    "CLASSMAP_JSON= OUT_DIR / \"class_maps.json\"\n",
    "METRICS_JSON = OUT_DIR / \"metrics_multitask.json\"\n",
    "\n",
    "# Training config\n",
    "RANDOM_SEED = 42\n",
    "BATCH_SIZE  = 64\n",
    "EPOCHS      = 20\n",
    "LR          = 1e-3\n",
    "WEIGHT_DECAY= 1e-4\n",
    "HIDDEN      = 512\n",
    "DROPOUT     = 0.2\n",
    "device = torch.device(\"cpu\")  # keep CPU for stability\n",
    "\n",
    "# Targets\n",
    "REG_COLS = [\n",
    "    \"acousticness\",\"danceability\",\"energy\",\"instrumentalness\",\"liveness\",\n",
    "    \"loudness\",\"speechiness\",\"tempo\",\"valence\",\n",
    "    \"duration_ms\"  # weak signal (weâ€™ll downâ€‘weight)\n",
    "]\n",
    "CLS_COLS = [\"key\",\"mode\",\"time_signature\"]\n",
    "\n",
    "LOSS_WEIGHTS = {c:1.0 for c in REG_COLS}\n",
    "LOSS_WEIGHTS[\"duration_ms\"] = 0.2  # deemphasize duration (previews are ~30s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1dfdb6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After normalization & filtering, rows = 6000\n",
      "Key class examples: [np.int64(0), np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6), np.int64(7), np.int64(8), np.int64(9), np.int64(10), np.int64(11)]\n",
      "Mode class examples: [np.int64(0), np.int64(1)]\n",
      "Time sig examples: [np.int64(1), np.int64(3), np.int64(4), np.int64(5)]\n",
      "Class sizes: {'key': 12, 'mode': 2, 'time_signature': 4}\n",
      "Train size: 4800  Val size: 1200\n",
      "Embedding dim: 2049\n"
     ]
    }
   ],
   "source": [
    "# CELL 2 (FIXED): normalize label columns, standardize regression targets, build class maps\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_parquet(PARQUET)\n",
    "emb_cols = [c for c in df.columns if c.startswith(\"e\")]\n",
    "X = df[emb_cols].values.astype(\"float32\")\n",
    "\n",
    "# --- Helpers to normalize categorical labels ---\n",
    "\n",
    "KEY_TO_INT = {\n",
    "    \"C\":0, \"C#\":1, \"D\":2, \"D#\":3, \"E\":4, \"F\":5, \"F#\":6, \"G\":7, \"G#\":8, \"A\":9, \"A#\":10, \"B\":11\n",
    "}\n",
    "FLAT_TO_SHARP = {\"DB\":\"C#\", \"EB\":\"D#\", \"GB\":\"F#\", \"AB\":\"G#\", \"BB\":\"A#\"}\n",
    "\n",
    "def normalize_key(val):\n",
    "    \"\"\"\n",
    "    Accepts strings like 'C#', 'Db', 'Aâ™­', 'C#/Db', 'C major', 'g#', etc.\n",
    "    Returns int 0..11 or np.nan.\n",
    "    \"\"\"\n",
    "    if pd.isna(val): return np.nan\n",
    "    s = str(val).strip()\n",
    "    # unify unicode accidentals\n",
    "    s = s.replace(\"â™¯\", \"#\").replace(\"â™­\", \"b\")\n",
    "    # take first token (drop things like \"C major\" or \"C#/Db\")\n",
    "    s = re.split(r\"[\\/\\s]+\", s)[0]\n",
    "    s = s.upper()\n",
    "\n",
    "    # map flats to sharps\n",
    "    if s in FLAT_TO_SHARP:\n",
    "        s = FLAT_TO_SHARP[s]\n",
    "\n",
    "    # sometimes datasets include weird tags like 'N' or '-1'\n",
    "    if s in KEY_TO_INT:\n",
    "        return KEY_TO_INT[s]\n",
    "\n",
    "    # if it already looks numeric (e.g., '0'..'11'), keep it\n",
    "    try:\n",
    "        v = int(s)\n",
    "        if 0 <= v <= 11:\n",
    "            return v\n",
    "    except Exception:\n",
    "        pass\n",
    "    return np.nan\n",
    "\n",
    "def normalize_mode(val):\n",
    "    \"\"\"\n",
    "    Accepts 'Major'/'Minor', 'major'/'minor', 1/0, '1'/'0'.\n",
    "    Spotify convention: major=1, minor=0.\n",
    "    \"\"\"\n",
    "    if pd.isna(val): return np.nan\n",
    "    s = str(val).strip().lower()\n",
    "    if s in {\"major\", \"maj\"}: return 1\n",
    "    if s in {\"minor\", \"min\"}: return 0\n",
    "    # numeric-ish\n",
    "    try:\n",
    "        v = int(float(s))\n",
    "        if v in (0, 1): return v\n",
    "    except Exception:\n",
    "        pass\n",
    "    return np.nan\n",
    "\n",
    "def normalize_time_signature(val):\n",
    "    \"\"\"\n",
    "    Accepts '4/4', '3/4', '5', 4, etc. Returns int top-number (e.g., 4).\n",
    "    \"\"\"\n",
    "    if pd.isna(val): return np.nan\n",
    "    s = str(val).strip()\n",
    "    m = re.search(r\"\\d+\", s)\n",
    "    if m:\n",
    "        try:\n",
    "            return int(m.group(0))\n",
    "        except Exception:\n",
    "            return np.nan\n",
    "    return np.nan\n",
    "\n",
    "# --- Apply normalization to categorical targets ---\n",
    "df[\"key_norm\"]  = df[\"key\"].apply(normalize_key)              if \"key\" in df.columns  else np.nan\n",
    "df[\"mode_norm\"] = df[\"mode\"].apply(normalize_mode)            if \"mode\" in df.columns else np.nan\n",
    "df[\"tsig_norm\"] = df[\"time_signature\"].apply(normalize_time_signature) if \"time_signature\" in df.columns else np.nan\n",
    "\n",
    "# --- Build mask for rows that have all required targets present ---\n",
    "needed = REG_COLS + [\"key_norm\", \"mode_norm\", \"tsig_norm\"]\n",
    "mask = df[needed].notna().all(axis=1)\n",
    "df = df[mask].reset_index(drop=True)\n",
    "\n",
    "# Update X after filtering\n",
    "X = df[emb_cols].values.astype(\"float32\")\n",
    "\n",
    "print(f\"After normalization & filtering, rows = {len(df)}\")\n",
    "print(\"Key class examples:\", sorted(pd.unique(df['key_norm']))[:12])\n",
    "print(\"Mode class examples:\", sorted(pd.unique(df['mode_norm'])))\n",
    "print(\"Time sig examples:\", sorted(pd.unique(df['tsig_norm'])))\n",
    "\n",
    "# --- Standardize regression targets (with safe transforms) ---\n",
    "reg_means, reg_stds, Y_reg = {}, {}, {}\n",
    "for c in REG_COLS:\n",
    "    y = df[c].astype(\"float32\").values\n",
    "    # transforms for skewed targets\n",
    "    if c == \"tempo\":\n",
    "        y = np.log1p(np.clip(y, 0, None))         # log(BPM)\n",
    "    if c == \"duration_ms\":\n",
    "        y = np.log1p(np.clip(y, 1.0, None))       # previews make this weak; still normalize\n",
    "    # loudness: already in dB; just standardize\n",
    "    m, s = float(y.mean()), float(y.std() + 1e-8)\n",
    "    reg_means[c], reg_stds[c] = m, s\n",
    "    Y_reg[c] = ((y - m) / s).astype(\"float32\")\n",
    "\n",
    "# --- Build class maps (value -> index) for categorical targets ---\n",
    "class_maps, Y_cls = {}, {}\n",
    "\n",
    "# key\n",
    "key_classes = sorted(pd.unique(df[\"key_norm\"]).astype(int).tolist())\n",
    "key_c2i = {v:i for i,v in enumerate(key_classes)}\n",
    "class_maps[\"key\"] = {\"classes\": key_classes, \"cls_to_idx\": key_c2i}\n",
    "Y_cls[\"key\"] = np.array([key_c2i[int(v)] for v in df[\"key_norm\"].values], dtype=\"int64\")\n",
    "\n",
    "# mode\n",
    "mode_classes = sorted(pd.unique(df[\"mode_norm\"]).astype(int).tolist())\n",
    "mode_c2i = {v:i for i,v in enumerate(mode_classes)}\n",
    "class_maps[\"mode\"] = {\"classes\": mode_classes, \"cls_to_idx\": mode_c2i}\n",
    "Y_cls[\"mode\"] = np.array([mode_c2i[int(v)] for v in df[\"mode_norm\"].values], dtype=\"int64\")\n",
    "\n",
    "# time_signature\n",
    "tsig_classes = sorted(pd.unique(df[\"tsig_norm\"]).astype(int).tolist())\n",
    "tsig_c2i = {v:i for i,v in enumerate(tsig_classes)}\n",
    "class_maps[\"time_signature\"] = {\"classes\": tsig_classes, \"cls_to_idx\": tsig_c2i}\n",
    "Y_cls[\"time_signature\"] = np.array([tsig_c2i[int(v)] for v in df[\"tsig_norm\"].values], dtype=\"int64\")\n",
    "\n",
    "print(\"Class sizes:\", {k: len(v[\"classes\"]) for k,v in class_maps.items()})\n",
    "\n",
    "# --- Train/val split (stratify by key to keep pitch distribution balanced) ---\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_tr, X_va, idx_tr, idx_va = train_test_split(\n",
    "    X, np.arange(len(X)), test_size=0.2, random_state=RANDOM_SEED, stratify=Y_cls[\"key\"]\n",
    ")\n",
    "\n",
    "# Helper to split dicts of arrays based on indices\n",
    "def split_dict(d, tr_idx, va_idx):\n",
    "    return {k: v[tr_idx] for k,v in d.items()}, {k: v[va_idx] for k,v in d.items()}\n",
    "\n",
    "Yreg_tr, Yreg_va = split_dict(Y_reg, idx_tr, idx_va)\n",
    "Ycls_tr, Ycls_va = split_dict(Y_cls, idx_tr, idx_va)\n",
    "\n",
    "print(\"Train size:\", len(idx_tr), \" Val size:\", len(idx_va))\n",
    "print(\"Embedding dim:\", X.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "964323f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MTModel(\n",
      "  (trunk): Sequential(\n",
      "    (0): Linear(in_features=2049, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.2, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (4): ReLU()\n",
      "  )\n",
      "  (reg_heads): ModuleDict(\n",
      "    (acousticness): HeadReg(\n",
      "      (fc): Linear(in_features=256, out_features=1, bias=True)\n",
      "    )\n",
      "    (danceability): HeadReg(\n",
      "      (fc): Linear(in_features=256, out_features=1, bias=True)\n",
      "    )\n",
      "    (energy): HeadReg(\n",
      "      (fc): Linear(in_features=256, out_features=1, bias=True)\n",
      "    )\n",
      "    (instrumentalness): HeadReg(\n",
      "      (fc): Linear(in_features=256, out_features=1, bias=True)\n",
      "    )\n",
      "    (liveness): HeadReg(\n",
      "      (fc): Linear(in_features=256, out_features=1, bias=True)\n",
      "    )\n",
      "    (loudness): HeadReg(\n",
      "      (fc): Linear(in_features=256, out_features=1, bias=True)\n",
      "    )\n",
      "    (speechiness): HeadReg(\n",
      "      (fc): Linear(in_features=256, out_features=1, bias=True)\n",
      "    )\n",
      "    (tempo): HeadReg(\n",
      "      (fc): Linear(in_features=256, out_features=1, bias=True)\n",
      "    )\n",
      "    (valence): HeadReg(\n",
      "      (fc): Linear(in_features=256, out_features=1, bias=True)\n",
      "    )\n",
      "    (duration_ms): HeadReg(\n",
      "      (fc): Linear(in_features=256, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (cls_heads): ModuleDict(\n",
      "    (key): HeadCls(\n",
      "      (fc): Linear(in_features=256, out_features=12, bias=True)\n",
      "    )\n",
      "    (mode): HeadCls(\n",
      "      (fc): Linear(in_features=256, out_features=2, bias=True)\n",
      "    )\n",
      "    (time_signature): HeadCls(\n",
      "      (fc): Linear(in_features=256, out_features=4, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# CELL 3: PyTorch Dataset/DataLoader and multi-task model definition\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# --- Dataset that returns x plus each target by name ---\n",
    "class MTSet(Dataset):\n",
    "    def __init__(self, X, yreg, ycls):\n",
    "        self.X = X\n",
    "        self.yreg = yreg\n",
    "        self.ycls = ycls\n",
    "        self.reg_cols = list(yreg.keys())\n",
    "        self.cls_cols = list(ycls.keys())\n",
    "    def __len__(self): \n",
    "        return len(self.X)\n",
    "    def __getitem__(self, i):\n",
    "        item = {\"x\": self.X[i]}\n",
    "        # standardized regression targets\n",
    "        for c in self.reg_cols:\n",
    "            item[f\"reg_{c}\"] = self.yreg[c][i]\n",
    "        # class indices for classification targets\n",
    "        for c in self.cls_cols:\n",
    "            item[f\"cls_{c}\"] = self.ycls[c][i]\n",
    "        return item\n",
    "\n",
    "train_ds = MTSet(X_tr, Yreg_tr, Ycls_tr)\n",
    "val_ds   = MTSet(X_va, Yreg_va, Ycls_va)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=False)\n",
    "val_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
    "\n",
    "# --- Simple heads ---\n",
    "class HeadReg(nn.Module):\n",
    "    def __init__(self, d_in):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(d_in, 1)\n",
    "    def forward(self, h):\n",
    "        return self.fc(h).squeeze(1)  # shape: (B,)\n",
    "\n",
    "class HeadCls(nn.Module):\n",
    "    def __init__(self, d_in, n_classes):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(d_in, n_classes)\n",
    "    def forward(self, h):\n",
    "        return self.fc(h)  # logits shape: (B, C)\n",
    "\n",
    "# --- Multi-task model: shared trunk + per-task heads ---\n",
    "class MTModel(nn.Module):\n",
    "    def __init__(self, d_in, hidden, dropout, class_maps, reg_cols):\n",
    "        super().__init__()\n",
    "        self.trunk = nn.Sequential(\n",
    "            nn.Linear(d_in, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden, hidden // 2),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        h = hidden // 2\n",
    "        self.reg_cols = list(reg_cols)\n",
    "        self.cls_cols = list(class_maps.keys())\n",
    "        # create a tiny head per target\n",
    "        self.reg_heads = nn.ModuleDict({c: HeadReg(h) for c in self.reg_cols})\n",
    "        self.cls_heads = nn.ModuleDict({c: HeadCls(h, len(class_maps[c][\"classes\"])) for c in self.cls_cols})\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.trunk(x)\n",
    "        out = {}\n",
    "        for c, head in self.reg_heads.items():\n",
    "            out[f\"reg_{c}\"] = head(h)\n",
    "        for c, head in self.cls_heads.items():\n",
    "            out[f\"cls_{c}\"] = head(h)\n",
    "        return out\n",
    "\n",
    "# Instantiate model & training components\n",
    "device = torch.device(\"cpu\")  # keep CPU for stability on M1\n",
    "model = MTModel(X.shape[1], HIDDEN, DROPOUT, class_maps, REG_COLS).to(device)\n",
    "\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "reg_loss = nn.SmoothL1Loss()\n",
    "cls_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f654c26d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | train 5.8961 | val 5.4085 | key_ACC 0.138 | mode_ACC 0.664 | time_sig_ACC 0.881\n",
      "Epoch 02 | train 5.4147 | val 5.3610 | key_ACC 0.143 | mode_ACC 0.665 | time_sig_ACC 0.877\n",
      "Epoch 03 | train 5.3268 | val 5.3159 | key_ACC 0.132 | mode_ACC 0.653 | time_sig_ACC 0.874\n",
      "Epoch 04 | train 5.2712 | val 5.2869 | key_ACC 0.146 | mode_ACC 0.660 | time_sig_ACC 0.874\n",
      "Epoch 05 | train 5.1993 | val 5.2839 | key_ACC 0.147 | mode_ACC 0.665 | time_sig_ACC 0.874\n",
      "Epoch 06 | train 5.1462 | val 5.2250 | key_ACC 0.147 | mode_ACC 0.673 | time_sig_ACC 0.876\n",
      "Epoch 07 | train 5.0864 | val 5.2995 | key_ACC 0.147 | mode_ACC 0.634 | time_sig_ACC 0.873\n",
      "Epoch 08 | train 5.0454 | val 5.1902 | key_ACC 0.138 | mode_ACC 0.655 | time_sig_ACC 0.880\n",
      "Epoch 09 | train 4.9868 | val 5.1525 | key_ACC 0.143 | mode_ACC 0.673 | time_sig_ACC 0.880\n",
      "Epoch 10 | train 4.9441 | val 5.1726 | key_ACC 0.131 | mode_ACC 0.637 | time_sig_ACC 0.874\n",
      "Epoch 11 | train 4.9035 | val 5.1434 | key_ACC 0.128 | mode_ACC 0.667 | time_sig_ACC 0.876\n",
      "Epoch 12 | train 4.8622 | val 5.1683 | key_ACC 0.143 | mode_ACC 0.655 | time_sig_ACC 0.876\n",
      "Epoch 13 | train 4.8378 | val 5.1705 | key_ACC 0.133 | mode_ACC 0.667 | time_sig_ACC 0.863\n",
      "Epoch 14 | train 4.7894 | val 5.1458 | key_ACC 0.129 | mode_ACC 0.644 | time_sig_ACC 0.877\n",
      "Epoch 15 | train 4.7610 | val 5.1928 | key_ACC 0.136 | mode_ACC 0.618 | time_sig_ACC 0.862\n",
      "Epoch 16 | train 4.7354 | val 5.1741 | key_ACC 0.138 | mode_ACC 0.659 | time_sig_ACC 0.875\n",
      "Epoch 17 | train 4.6920 | val 5.1993 | key_ACC 0.134 | mode_ACC 0.657 | time_sig_ACC 0.870\n",
      "Epoch 18 | train 4.6433 | val 5.2034 | key_ACC 0.127 | mode_ACC 0.653 | time_sig_ACC 0.864\n",
      "Epoch 19 | train 4.6159 | val 5.2473 | key_ACC 0.113 | mode_ACC 0.609 | time_sig_ACC 0.872\n",
      "Epoch 20 | train 4.5737 | val 5.2432 | key_ACC 0.126 | mode_ACC 0.614 | time_sig_ACC 0.868\n",
      "\n",
      "Best (by val loss): {'epoch': 11, 'loss': 5.143377691904703, 'acousticness_MAE': 0.12975649535655975, 'acousticness_R2': 0.7344821691513062, 'danceability_MAE': 0.09475541114807129, 'danceability_R2': 0.5750526189804077, 'energy_MAE': 0.05144298076629639, 'energy_R2': 0.9372408390045166, 'instrumentalness_MAE': 0.10054696351289749, 'instrumentalness_R2': 0.589205265045166, 'liveness_MAE': 0.10224820673465729, 'liveness_R2': 0.2870725393295288, 'loudness_MAE': 1.9015880823135376, 'loudness_R2': 0.8087679743766785, 'speechiness_MAE': 0.05239148810505867, 'speechiness_R2': 0.7621148824691772, 'tempo_MAE': 23.72967529296875, 'tempo_R2': 0.0690758228302002, 'valence_MAE': 0.14353075623512268, 'valence_R2': 0.5065160989761353, 'duration_ms_MAE': 59135.19140625, 'duration_ms_R2': -0.015391945838928223, 'key_ACC': 0.12833333333333333, 'mode_ACC': 0.6675, 'time_signature_ACC': 0.8758333333333334}\n"
     ]
    }
   ],
   "source": [
    "# CELL 4: training + validation loop with per-target metrics and best-model saving\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, accuracy_score\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def train_epoch():\n",
    "    model.train()\n",
    "    total_loss, total_count = 0.0, 0\n",
    "    for batch in train_loader:\n",
    "        x = torch.as_tensor(batch[\"x\"], dtype=torch.float32, device=device)\n",
    "        out = model(x)\n",
    "\n",
    "        # total multi-task loss = sum(regression heads) + sum(classification heads)\n",
    "        loss = 0.0\n",
    "        # regression heads: standardized targets\n",
    "        for c in REG_COLS:\n",
    "            y = torch.as_tensor(batch[f\"reg_{c}\"], dtype=torch.float32, device=device)\n",
    "            loss = loss + LOSS_WEIGHTS[c] * reg_loss(out[f\"reg_{c}\"], y)\n",
    "        # classification heads: class indices\n",
    "        for c in CLS_COLS:\n",
    "            y = torch.as_tensor(batch[f\"cls_{c}\"], dtype=torch.long, device=device)\n",
    "            loss = loss + cls_loss(out[f\"cls_{c}\"], y)\n",
    "\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        total_loss += loss.item() * x.size(0)\n",
    "        total_count += x.size(0)\n",
    "    return total_loss / total_count\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_epoch():\n",
    "    model.eval()\n",
    "    total_loss, total_count = 0.0, 0\n",
    "\n",
    "    # store predictions/targets to compute metrics after the loop\n",
    "    preds_reg = {c: [] for c in REG_COLS}\n",
    "    trues_reg = {c: [] for c in REG_COLS}\n",
    "    preds_cls = {c: [] for c in CLS_COLS}\n",
    "    trues_cls = {c: [] for c in CLS_COLS}\n",
    "\n",
    "    for batch in val_loader:\n",
    "        x = torch.as_tensor(batch[\"x\"], dtype=torch.float32, device=device)\n",
    "        out = model(x)\n",
    "\n",
    "        loss = 0.0\n",
    "        for c in REG_COLS:\n",
    "            y = torch.as_tensor(batch[f\"reg_{c}\"], dtype=torch.float32, device=device)\n",
    "            p = out[f\"reg_{c}\"]\n",
    "            loss += LOSS_WEIGHTS[c] * reg_loss(p, y)\n",
    "            preds_reg[c].append(p.cpu().numpy())\n",
    "            trues_reg[c].append(y.cpu().numpy())\n",
    "\n",
    "        for c in CLS_COLS:\n",
    "            y = torch.as_tensor(batch[f\"cls_{c}\"], dtype=torch.long, device=device)\n",
    "            p = out[f\"cls_{c}\"]\n",
    "            loss += cls_loss(p, y)\n",
    "            preds_cls[c].append(p.argmax(1).cpu().numpy())\n",
    "            trues_cls[c].append(y.cpu().numpy())\n",
    "\n",
    "        total_loss += loss.item() * x.size(0)\n",
    "        total_count += x.size(0)\n",
    "\n",
    "    # aggregate metrics\n",
    "    metrics = {\"loss\": total_loss / total_count}\n",
    "\n",
    "    # Regression metrics in ORIGINAL units: invert standardization & any log transforms\n",
    "    for c in REG_COLS:\n",
    "        y_std = np.concatenate(trues_reg[c])\n",
    "        p_std = np.concatenate(preds_reg[c])\n",
    "        m, s = reg_means[c], reg_stds[c]\n",
    "        y = y_std * s + m\n",
    "        p = p_std * s + m\n",
    "        if c == \"tempo\":\n",
    "            y = np.expm1(y); p = np.expm1(p)\n",
    "        if c == \"duration_ms\":\n",
    "            y = np.expm1(y); p = np.expm1(p)\n",
    "        metrics[f\"{c}_MAE\"] = float(mean_absolute_error(y, p))\n",
    "        metrics[f\"{c}_R2\"]  = float(r2_score(y, p))\n",
    "\n",
    "    # Classification accuracy\n",
    "    for c in CLS_COLS:\n",
    "        y = np.concatenate(trues_cls[c])\n",
    "        p = np.concatenate(preds_cls[c])\n",
    "        metrics[f\"{c}_ACC\"] = float(accuracy_score(y, p))\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "# ---- Run training ----\n",
    "best = None\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    tr_loss = train_epoch()\n",
    "    val = eval_epoch()\n",
    "\n",
    "    # concise status line (you can print more metrics if you want)\n",
    "    status = (f\"Epoch {epoch:02d} | train {tr_loss:.4f} | val {val['loss']:.4f} | \"\n",
    "              f\"key_ACC {val['key_ACC']:.3f} | mode_ACC {val['mode_ACC']:.3f} | \"\n",
    "              f\"time_sig_ACC {val['time_signature_ACC']:.3f}\")\n",
    "    print(status)\n",
    "\n",
    "    # save best by validation loss\n",
    "    if best is None or val[\"loss\"] < best[\"loss\"]:\n",
    "        best = {\"epoch\": epoch, **val}\n",
    "        torch.save(model.state_dict(), MODEL_PATH)\n",
    "\n",
    "print(\"\\nBest (by val loss):\", best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02c66dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’¾ Saved model:      /Users/prajeetdarda/Desktop/All_Coding/AI-Project/web/model-training/output/multitask_head.pt\n",
      "ðŸ’¾ Saved scalers:    /Users/prajeetdarda/Desktop/All_Coding/AI-Project/web/model-training/output/regression_scalers.json\n",
      "ðŸ’¾ Saved class maps: /Users/prajeetdarda/Desktop/All_Coding/AI-Project/web/model-training/output/class_maps.json\n",
      "ðŸ“ˆ Metrics:          /Users/prajeetdarda/Desktop/All_Coding/AI-Project/web/model-training/output/metrics_multitask.json\n"
     ]
    }
   ],
   "source": [
    "# CELL 5: save scalers, class maps, and best metrics for inference\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Save regression scalers (mean/std per target) for de-standardization at inference\n",
    "with open(SCALERS_JSON, \"w\") as f:\n",
    "    json.dump({\"mean\": reg_means, \"std\": reg_stds}, f, indent=2)\n",
    "\n",
    "# Save class maps (actual class values â†” indices)\n",
    "with open(CLASSMAP_JSON, \"w\") as f:\n",
    "    json.dump(class_maps, f, indent=2)\n",
    "\n",
    "# Save metrics summary\n",
    "with open(METRICS_JSON, \"w\") as f:\n",
    "    json.dump(best, f, indent=2)\n",
    "\n",
    "print(f\"ðŸ’¾ Saved model:      {MODEL_PATH}\")\n",
    "print(f\"ðŸ’¾ Saved scalers:    {SCALERS_JSON}\")\n",
    "print(f\"ðŸ’¾ Saved class maps: {CLASSMAP_JSON}\")\n",
    "print(f\"ðŸ“ˆ Metrics:          {METRICS_JSON}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47c60a3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acousticness': 0.6405558586120605,\n",
       " 'danceability': 0.515924334526062,\n",
       " 'energy': 0.36192819476127625,\n",
       " 'instrumentalness': 0.21889616549015045,\n",
       " 'liveness': 0.11445026844739914,\n",
       " 'loudness': -11.277266502380371,\n",
       " 'speechiness': 0.03460095077753067,\n",
       " 'tempo': 118.63714923378535,\n",
       " 'valence': 0.3236296772956848,\n",
       " 'duration_ms': 226190.83729823618,\n",
       " 'key': 0,\n",
       " 'mode': 0,\n",
       " 'time_signature': 4}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CELL 6: quick prediction on one validation sample (emb â†’ feature dict)\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_features_from_emb(emb_vec):\n",
    "    x = torch.as_tensor(emb_vec[None, :], dtype=torch.float32, device=device)\n",
    "    model.eval()\n",
    "    out = model(x)\n",
    "    # regressions\n",
    "    reg_pred = {}\n",
    "    for c in REG_COLS:\n",
    "        p_std = out[f\"reg_{c}\"].cpu().numpy()[0]\n",
    "        m, s = reg_means[c], reg_stds[c]\n",
    "        val = float(p_std * s + m)\n",
    "        if c == \"tempo\":        val = float(np.expm1(val))\n",
    "        if c == \"duration_ms\":  val = float(np.expm1(val))\n",
    "        reg_pred[c] = val\n",
    "    # classifications\n",
    "    cls_pred = {}\n",
    "    for c in CLS_COLS:\n",
    "        logits = out[f\"cls_{c}\"].cpu().numpy()[0]\n",
    "        idx = int(np.argmax(logits))\n",
    "        cls_val = class_maps[c][\"classes\"][idx]\n",
    "        cls_pred[c] = int(cls_val)\n",
    "    return {**reg_pred, **cls_pred}\n",
    "\n",
    "sample_pred = predict_features_from_emb(X_va[0])\n",
    "sample_pred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
